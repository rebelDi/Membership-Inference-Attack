{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging.config import DEFAULT_LOGGING_CONFIG_PORT\n",
    "import os\n",
    "from pyexpat import model\n",
    "import torch\n",
    "import pandas\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from pynvml import *\n",
    "from typing import Any, Callable, List, Optional, Union, Tuple\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import ndimage, stats\n",
    "from numpy.core.fromnumeric import mean\n",
    "from opacus import PrivacyEngine\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to gpu and get number of available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--->\", torch.cuda.device_count())\n",
    "device_number = int(sys.argv[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to understand how much memory the code consumes from CUDA\n",
    "\n",
    "def get_memory():\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(device_number)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    print(\"\\n_______ MEMORY _______\")\n",
    "    print(f'total    : {info.total}')\n",
    "    print(f'free     : {info.free}')\n",
    "    print(f'used     : {info.used}\\n')\n",
    "\n",
    "get_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes: int = 1000, dropout: float = 0.5) -> None:\n",
    "        super(AlexNet, self).__init__()\n",
    "        # _log_api_usage_once(self)\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=11, stride=4, padding=2),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                      nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                      nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                      )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "                                        nn.Dropout(p=dropout),\n",
    "                                        nn.Linear(256 * 6 * 6, 4096),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(p=dropout),\n",
    "                                        nn.Linear(4096, 512),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(512, num_classes),\n",
    "                                        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip=None\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep=[]\n",
    "\n",
    "        filters=in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "        \n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x+=skip\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception optimized for the ImageNet dataset, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, num_classes=1000, dropout=0.5):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, 3, 2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #do relu here\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        #do relu here\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Dropout(p=dropout),                    \n",
    "                                nn.Linear(2048, 1024),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(1024, 512),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Linear(512, num_classes),\n",
    "                                nn.Softmax(dim=1),\n",
    "                                )\n",
    "\n",
    "        #------- init weights --------\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        #-----------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def target_model_fn(input_channel=1, num_classes=10):\n",
    "    global model_architecture\n",
    "\n",
    "    if model_architecture == \"resnet18\":\n",
    "    # load resnet 18 and change to fit problem dimensionality\n",
    "        model = models.resnet18()\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "        model.fc = nn.Sequential(nn.Linear(512, num_classes), nn.LogSoftmax(dim=1))\n",
    "    elif model_architecture == \"alexnet\":\n",
    "        model = AlexNet(in_channels=input_channel, num_classes=num_classes)\n",
    "    elif model_architecture == \"xception\":\n",
    "        model = Xception(in_channels=input_channel, num_classes=num_classes)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses a lot of CUDA memory, in order to get CUDA out of memory error less frequent we use the next method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    with torch.cuda.device(f'cuda:{device_number}'):\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{device_number}\"\n",
    "device = torch.device(f\"cuda:{device_number}\")\n",
    "\n",
    "def clear_cache():\n",
    "    with torch.cuda.device(f'cuda:{device_number}'):\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Imported all libraries\")\n",
    "clear_cache()\n",
    "get_memory()\n",
    "\n",
    "MEAN_DATA = 0\n",
    "STD_DATA = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CelebA(torch.utils.data.Dataset):\n",
    "    base_folder = \"celebA\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            attr_list: str,\n",
    "            target_type: Union[List[str], str] = \"attr\",\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform =target_transform\n",
    "        self.attr_list = attr_list\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder+\"/processed\")\n",
    "        splits = pandas.read_csv(fn(\"list_eval_partition.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        attr = pandas.read_csv(fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "\n",
    "        mask = slice(None)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = torch.div(self.attr + 1, 2, rounding_mode='floor')  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        X = Image.open(os.path.join(self.root, self.base_folder, \"raw/img_celeba\", self.filename[index]))\n",
    "\n",
    "        target: Any = []\n",
    "        for t, nums in zip(self.target_type, self.attr_list):\n",
    "            if t == \"attr\":\n",
    "                final_attr = 0\n",
    "                for i in range(len(nums)):\n",
    "                    final_attr += 2 ** i * self.attr[index][nums[i]]\n",
    "                target.append(final_attr)\n",
    "            else:\n",
    "                # TODO: refactor with utils.verify_str_arg\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.attr)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)\n",
    "\n",
    "from typing import Any, Callable, List, Optional, Union, Tuple\n",
    "from PIL import Image\n",
    "class UTKFaceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, attr: Union[List[str], str] = \"gender\", transform=None, target_transform=None)-> None:\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.processed_path = os.path.join(self.root, \"data/UTKFace/processed\")\n",
    "        self.files = os.listdir(self.processed_path)\n",
    "        if isinstance(attr, list):\n",
    "            self.attr = attr\n",
    "        else:\n",
    "            self.attr = [attr]\n",
    "\n",
    "        self.lines = []\n",
    "        for txt_file in self.files:\n",
    "            txt_file_path = os.path.join(self.processed_path, txt_file)\n",
    "            print(txt_file_path)\n",
    "            with open(txt_file_path, 'r') as f:\n",
    "                assert f is not None\n",
    "                for i in f:\n",
    "                    image_name = i.split('jpg ')[0]\n",
    "                    attrs = image_name.split('_')\n",
    "                    if len(attrs) < 4 or int(attrs[2]) >= 4  or '' in attrs:\n",
    "                        continue\n",
    "                    self.lines.append(image_name+'jpg')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, index:int)-> Tuple[Any, Any]:\n",
    "        attrs = self.lines[index].split('_')\n",
    "\n",
    "        age = int(attrs[0])\n",
    "        gender = int(attrs[1])\n",
    "        race = int(attrs[2])\n",
    "\n",
    "        image_path = os.path.join(self.root, 'data/UTKFace/raw', self.lines[index]+'.chip.jpg').rstrip()\n",
    "  \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        target: Any = []\n",
    "        for t in self.attr:\n",
    "            if t == \"age\":\n",
    "                target.append(age)\n",
    "            elif t == \"gender\":\n",
    "                target.append(gender)\n",
    "            elif t == \"race\":\n",
    "                target.append(race)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return image, target\n",
    "\n",
    "        \n",
    "def get_model_dataset(dataset_name, attr, root):\n",
    "    global num_classes, MEAN_DATA, STD_DATA\n",
    "\n",
    "    if dataset_name.lower() == \"utkface\":\n",
    "        if isinstance(attr, list):\n",
    "            num_classes = []\n",
    "            for a in attr:\n",
    "                if a == \"age\":\n",
    "                    num_classes.append(117)\n",
    "                elif a == \"gender\":\n",
    "                    num_classes.append(2)\n",
    "                elif a == \"race\":\n",
    "                    num_classes.append(4)\n",
    "                else:\n",
    "                    raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(a))\n",
    "        else:\n",
    "            if attr == \"age\":\n",
    "                num_classes = 117\n",
    "            elif attr == \"gender\":\n",
    "                num_classes = 2\n",
    "            elif attr == \"race\":\n",
    "                num_classes = 4\n",
    "            else:\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(attr))\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "        dataset = UTKFaceDataset(root=root, attr=attr, transform=transform)\n",
    "        input_channel = 3\n",
    "\n",
    "    elif dataset_name.lower() == \"stl10\":\n",
    "        num_classes = 10\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        print(\"Loading stl10\")\n",
    "\n",
    "        train_set = torchvision.datasets.STL10(\n",
    "                root=root, split='train', transform=transform, download=True)\n",
    "            \n",
    "        test_set = torchvision.datasets.STL10(\n",
    "                root=root, split='test', transform=transform, download=True)\n",
    "\n",
    "        dataset = train_set + test_set\n",
    "        input_channel = 3\n",
    "        \n",
    "    elif dataset_name.lower() == \"celeba\":\n",
    "        if isinstance(attr, list):\n",
    "            for a in attr:\n",
    "                if a != \"attr\":\n",
    "                    raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(a))\n",
    "\n",
    "                num_classes = [8, 4]\n",
    "                attr_list = [[18, 21, 31], [20, 39]]\n",
    "        else:\n",
    "            if attr == \"attr\":\n",
    "                num_classes = 8\n",
    "                attr_list = [[18, 21, 31]]\n",
    "            else:\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(attr))\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "        dataset = CelebA(root=root, attr_list=attr_list, target_type=attr, transform=transform)\n",
    "        input_channel = 3\n",
    "\n",
    "    elif dataset_name.lower() == \"fmnist\":\n",
    "        num_classes = 10\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "        train_set = torchvision.datasets.FashionMNIST(\n",
    "                root=root, train=True, download=True, transform=transform)\n",
    "        test_set = torchvision.datasets.FashionMNIST(\n",
    "                root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "        dataset = train_set + test_set\n",
    "        input_channel = 1\n",
    "\n",
    "    if isinstance(num_classes, int):\n",
    "        target_model, target_optim = target_model_fn(input_channel=input_channel, num_classes=num_classes)\n",
    "        shadow_model, shadow_optim = target_model_fn(input_channel=input_channel, num_classes=num_classes)\n",
    "    else:\n",
    "        target_model, target_optim = target_model_fn(input_channel=input_channel, num_classes=num_classes[0])\n",
    "        shadow_model, shadow_optim = target_model_fn(input_channel=input_channel, num_classes=num_classes[0])\n",
    "\n",
    "    return num_classes, dataset, target_model, shadow_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, attr, root):\n",
    "    num_classes, dataset, target_model, shadow_model = get_model_dataset(dataset, attr=attr, root=root)\n",
    "    length = len(dataset)\n",
    "    each_length = length//4\n",
    "    target_train, target_test, shadow_train, shadow_test, _ = torch.utils.data.random_split(dataset, [each_length, each_length, each_length, each_length, len(dataset)-(each_length*4)])\n",
    "    \n",
    "    return num_classes, target_train, target_test, shadow_train, shadow_test, target_model, shadow_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add to the data a noise based on the right variation and mean of the dataset, we declare global variables and assign them a value according to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_mean_std(dataset_name):\n",
    "    global MEAN_DATA, STD_DATA\n",
    "\n",
    "    if dataset_name == \"stl10\":\n",
    "        MEAN_DATA = np.mean([0.4914, 0.4822, 0.4465])\n",
    "        STD_DATA = np.mean([0.2023, 0.1994, 0.2010])        \n",
    "    elif dataset_name == \"utkface\":\n",
    "        MEAN_DATA = np.mean([0.5, 0.5, 0.5])\n",
    "        STD_DATA = np.mean([0.5, 0.5, 0.5])\n",
    "    elif dataset_name == \"fmnist\":\n",
    "        MEAN_DATA = 0.1307\n",
    "        STD_DATA = 0.3081\n",
    "    elif dataset_name == \"celeba\":\n",
    "        MEAN_DATA = np.mean([0.5, 0.5, 0.5])\n",
    "        STD_DATA = np.mean([0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section allows to choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr=\"race\"\n",
    "# dataset_name=\"stl10\"\n",
    "dataset_name = sys.argv[1]\n",
    "\n",
    "if dataset_name == \"stl10\":\n",
    "    num_classes = 10\n",
    "    input_channel = 3\n",
    "elif dataset_name == \"utkface\":\n",
    "    num_classes = 4\n",
    "    input_channel = 3\n",
    "elif dataset_name == \"fmnist\":\n",
    "    input_channel = 3\n",
    "    num_classes = 10\n",
    "\n",
    "root = \"data\" # where is data folder for datasets\n",
    "use_DP = 0\n",
    "noise = 0\n",
    "norm = 0\n",
    "delta = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "root_folder = \"\" # where all the outputs of the code are saved\n",
    "TARGET_PATH = f\"{root_folder}target_models/{dataset_name}/\"\n",
    "\n",
    "# model_architecture = \"resnet18\"\n",
    "# model_architecture = \"alexnet\"\n",
    "# model_architecture = \"xception\"\n",
    "model_architecture = sys.argv[2]\n",
    "\n",
    "# layers_neurons_percentage = 100\n",
    "# number_of_layers = 1\n",
    "layers_neurons_percentage = int(sys.argv[3])\n",
    "number_of_layers = int(sys.argv[4])\n",
    "\n",
    "# method = \"NoIntermediate\"\n",
    "# method = \"RandomForest\"\n",
    "# method = \"Ttest\"\n",
    "# method = \"KS2Samp\"\n",
    "# method = \"KLDivergence\"\n",
    "# method = \"Bootstrapping\"\n",
    "# method = \"Noise\"\n",
    "# method = \"JustNoise\"\n",
    "method = sys.argv[5]\n",
    "\n",
    "# These 3 need to only be calculated once\n",
    "datasets_ready = True\n",
    "pretrained = True\n",
    "test_models = False\n",
    "\n",
    "# recalculate = True # for each method only calculate this once\n",
    "recalculate = eval(sys.argv[6])\n",
    "\n",
    "# These 2 need to only be calculated once\n",
    "outputs_calculated = eval(sys.argv[7])\n",
    "noise_difference_calculated = eval(sys.argv[8])\n",
    "\n",
    "# If you got an error due to the attack model architecture, \n",
    "# assign this to True, it will allow to run the code faster\n",
    "prepare_dataset_pretrained = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code needs different folders to save results. Next function allows to create them all automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_create_folders():\n",
    "    all_paths = [f\"{root_folder}datasets\", f\"{root_folder}datasets/{dataset_name}\", f\"{root_folder}target_models\", \\\n",
    "        f\"{root_folder}target_models/{dataset_name}\", f\"{root_folder}attack\", f\"{root_folder}attack/{dataset_name}\", \\\n",
    "            f\"{root_folder}attack/{dataset_name}/{model_architecture}\", f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/\", \\\n",
    "                f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}\", f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs\"]\n",
    "\n",
    "    for path in all_paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "check_create_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not datasets_ready:\n",
    "    num_classes, target_train, target_test, shadow_train, shadow_test, target_model, shadow_model = prepare_dataset(dataset_name, attr, root)\n",
    "\n",
    "    print(\"Number of samples in training set:\", len(target_train))\n",
    "    print(\"Number of samples in test set:\", len(target_test))\n",
    "\n",
    "    torch.save(target_train, f\"{root_folder}datasets/\"+dataset_name+\"/in_data_target\")\n",
    "    torch.save(target_test, f\"{root_folder}datasets/\"+dataset_name+\"/out_data_target\")\n",
    "\n",
    "    torch.save(shadow_train, f\"{root_folder}datasets/\"+dataset_name+\"/in_data_shadow\")\n",
    "    torch.save(shadow_test, f\"{root_folder}datasets/\"+dataset_name+\"/out_data_shadow\")\n",
    "\n",
    "target_train = torch.load(f\"{root_folder}datasets/\"+dataset_name+\"/in_data_target\")\n",
    "target_test = torch.load(f\"{root_folder}datasets/\"+dataset_name+\"/out_data_target\")\n",
    "\n",
    "shadow_train = torch.load(f\"{root_folder}datasets/\"+dataset_name+\"/in_data_shadow\")\n",
    "shadow_test = torch.load(f\"{root_folder}datasets/\"+dataset_name+\"/out_data_shadow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(PATH, device, train_set, test_set, model, use_DP, noise, norm, delta):\n",
    "    batch_size = 64\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "    \n",
    "    model = model_training(train_loader, test_loader, model, device, use_DP, noise, norm, delta)\n",
    "    acc_train = 0\n",
    "    acc_test = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        print(\"<======================= Epoch \" + str(i+1) + \" =======================>\")\n",
    "        print(\"target training\")\n",
    "\n",
    "        acc_train = model.train()\n",
    "        print(\"target testing\")\n",
    "        acc_test = model.test()\n",
    "\n",
    "        overfitting = round(acc_train - acc_test, 6)\n",
    "        print('The overfitting rate is %s' % overfitting)\n",
    "\n",
    "    FILE_PATH = PATH + f\"{dataset_name}_{model_architecture}_target.pth\"\n",
    "    model.saveModel(FILE_PATH)\n",
    "    print(\"Saved target model!!!\")\n",
    "    print(\"Finished training!!!\")\n",
    "\n",
    "    return acc_train, acc_test, overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing method\n",
    "def test(model, loader, dname=\"Test set\", printable=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device) # send to device\n",
    "            output = model(data)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(loader.dataset)\n",
    "    if printable:\n",
    "        print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            dname, test_loss, correct, total, \n",
    "            100. * correct / total\n",
    "            ))\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for target model\n",
    "class model_training():\n",
    "    def __init__(self, trainloader, testloader, model, device, use_DP, noise, norm, delta):\n",
    "        self.use_DP = use_DP\n",
    "        self.device = device\n",
    "        self.delta = delta\n",
    "        self.net = model.to(self.device)\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.net = torch.nn.DataParallel(self.net)\n",
    "            cudnn.benchmark = True\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        self.noise_multiplier, self.max_grad_norm = noise, norm\n",
    "        \n",
    "        if self.use_DP:\n",
    "            self.privacy_engine = PrivacyEngine()\n",
    "            self.model, self.optimizer, self.trainloader = self.privacy_engine.make_private(\n",
    "                module=model,\n",
    "                optimizer=self.optimizer,\n",
    "                data_loader=self.trainloader,\n",
    "                noise_multiplier=self.noise_multiplier,\n",
    "                max_grad_norm=self.max_grad_norm,\n",
    "            )\n",
    "            print( 'noise_multiplier: %.3f | max_grad_norm: %.3f' % (self.noise_multiplier, self.max_grad_norm))\n",
    "\n",
    "\n",
    "    # Training\n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(self.trainloader):\n",
    "            if isinstance(targets, list):\n",
    "                targets = targets[0]\n",
    "\n",
    "            if str(self.criterion) != \"CrossEntropyLoss()\":\n",
    "                targets = torch.from_numpy(np.eye(self.num_classes)[targets]).float()\n",
    "             \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net(inputs)\n",
    "\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            if str(self.criterion) != \"CrossEntropyLoss()\":\n",
    "                _, targets= targets.max(1)\n",
    "\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if self.use_DP:\n",
    "            epsilon, best_alpha = self.privacy_engine.accountant.get_privacy_spent(delta=self.delta)\n",
    "            # epsilon, best_alpha = self.optimizer.privacy_engine.get_privacy_spent(1e-5)\n",
    "            print(\"\\u03B1: %.3f \\u03B5: %.3f \\u03B4: 1e-5\" % (best_alpha, epsilon))\n",
    "                \n",
    "        print( 'Train Acc: %.3f%% (%d/%d) | Loss: %.3f' % (100.*correct/total, correct, total, 1.*train_loss/batch_idx))\n",
    "\n",
    "        return 1.*correct/total\n",
    "\n",
    "    \n",
    "    def saveModel(self, path):\n",
    "        torch.save(self.net.state_dict(), path)\n",
    "\n",
    "    def get_noise_norm(self):\n",
    "        return self.noise_multiplier, self.max_grad_norm\n",
    "\n",
    "    def test(self):\n",
    "        self.net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.testloader:\n",
    "                if isinstance(targets, list):\n",
    "                    targets = targets[0]\n",
    "                if str(self.criterion) != \"CrossEntropyLoss()\":\n",
    "                    targets = torch.from_numpy(np.eye(self.num_classes)[targets]).float()\n",
    "\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.net(inputs)\n",
    "\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                if str(self.criterion) != \"CrossEntropyLoss()\":\n",
    "                    _, targets= targets.max(1)\n",
    "\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print( 'Test Acc: %.3f%% (%d/%d)' % (100.*correct/total, correct, total))\n",
    "\n",
    "        return 1.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model, _ = target_model_fn(input_channel, num_classes)\n",
    "\n",
    "# We only train the models if we don't have them pretrained\n",
    "if not pretrained:\n",
    "    train_model(TARGET_PATH, device, target_train, target_test, target_model, use_DP, noise, norm, delta)\n",
    "\n",
    "if test_models:\n",
    "    test_target_model, _ = target_model_fn(input_channel, num_classes)\n",
    "    test_target_model.load_state_dict(torch.load(TARGET_PATH + f\"{dataset_name}_{model_architecture}_target.pth\"))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        target_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        target_test, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    print(\"\\n-------------------------------------------------\")\n",
    "    test_target_model_train = model_training(train_loader, train_loader, test_target_model, device, use_DP, noise, norm, delta)\n",
    "    print(\"Target Model Train Acc:\", test_target_model_train.test())\n",
    "\n",
    "    test_target_model_test = model_training(train_loader, test_loader, test_target_model, device, use_DP, noise, norm, delta)\n",
    "    print(\"Target Model Test Acc:\", test_target_model_test.test())\n",
    "    print(\"\\n-------------------------------------------------\")\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadow Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for shadow model\n",
    "class shadow():\n",
    "    def __init__(self, trainloader, testloader, model, device, use_DP, noise, norm, loss, optimizer, delta):\n",
    "        self.delta = delta\n",
    "        self.use_DP = use_DP\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "\n",
    "        self.criterion = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.noise_multiplier, self.max_grad_norm = noise, norm\n",
    "        \n",
    "        if self.use_DP:\n",
    "            self.privacy_engine = PrivacyEngine()\n",
    "            self.model, self.optimizer, self.trainloader = self.privacy_engine.make_private(\n",
    "                module=self.model,\n",
    "                optimizer=self.optimizer,\n",
    "                data_loader=self.trainloader,\n",
    "                noise_multiplier=self.noise_multiplier,\n",
    "                max_grad_norm=self.max_grad_norm,\n",
    "            )\n",
    "            print( 'noise_multiplier: %.3f | max_grad_norm: %.3f' % (self.noise_multiplier, self.max_grad_norm))\n",
    "            \n",
    "        self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, [50, 100], 0.1)\n",
    "\n",
    "    # Training\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(self.trainloader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if self.use_DP:\n",
    "            epsilon, best_alpha = self.privacy_engine.accountant.get_privacy_spent(delta=self.delta)\n",
    "            # epsilon, best_alpha = self.optimizer.privacy_engine.get_privacy_spent(1e-5)\n",
    "            print(\"\\u03B1: %.3f \\u03B5: %.3f \\u03B4: 1e-5\" % (best_alpha, epsilon))\n",
    "                \n",
    "        print( 'Train Acc: %.3f%% (%d/%d) | Loss: %.3f' % (100.*correct/total, correct, total, 1.*train_loss/batch_idx))\n",
    "\n",
    "        return 1.*correct/total\n",
    "\n",
    "\n",
    "    def saveModel(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def get_noise_norm(self):\n",
    "        return self.noise_multiplier, self.max_grad_norm\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.testloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print( 'Test Acc: %.3f%% (%d/%d)' % (100.*correct/total, correct, total))\n",
    "\n",
    "        return 1.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_shadow_model(PATH, device, shadow_model, train_loader, test_loader, use_DP, noise, norm, loss, optimizer, delta):\n",
    "    model = shadow(train_loader, test_loader, shadow_model, device, use_DP, noise, norm, loss, optimizer, delta)\n",
    "    acc_train = 0\n",
    "    acc_test = 0\n",
    "    batch_size = 64\n",
    "\n",
    "    if early_stop:\n",
    "        # to track the average training loss per epoch as the model trains\n",
    "        avg_train_losses = []\n",
    "        # to track the average validation loss per epoch as the model trains\n",
    "        avg_valid_losses = [] \n",
    "        train_loader, valid_loader = create_datasets(batch_size, shadow_train)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    for i in range(100):\n",
    "        print(\"<======================= Epoch \" + str(i+1) + \" =======================>\")\n",
    "        print(\"shadow training\")\n",
    "\n",
    "        if early_stop:\n",
    "            print(f\"\\rEpoch {i}  \"  , end=\"\")\n",
    "            shadow_model, avg_train_losses, avg_valid_losses, return_cond \\\n",
    "                = model.train_early_stop(early_stopping, avg_train_losses, avg_valid_losses, \\\n",
    "                    shadow_model, model.optimizer, i, train_loader, valid_loader, returnable=True)\n",
    "\n",
    "            print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            print(\"\\Shadow Model train accuracy: \")\n",
    "            acc_train = model.train()\n",
    "            print(acc_train)\n",
    "            print(\"\\Shadow Model Test Accuracy\")\n",
    "            acc_test = model.test()\n",
    "            print(acc_test)\n",
    "\n",
    "            if return_cond:\n",
    "                break\n",
    "        else:\n",
    "            acc_train = model.train()\n",
    "            print(\"shadow testing\")\n",
    "            acc_test = model.test()\n",
    "\n",
    "        overfitting = round(acc_train - acc_test, 6)\n",
    "\n",
    "        print('The overfitting rate is %s' % overfitting)\n",
    "\n",
    "    FILE_PATH = TARGET_PATH + f\"{dataset_name}_{model_architecture}_shadow.pth\"\n",
    "    model.saveModel(FILE_PATH)\n",
    "    print(\"saved shadow model!!!\")\n",
    "    print(\"Finished training!!!\")\n",
    "\n",
    "    return acc_train, acc_test, overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "shadow_model, _ = target_model_fn(input_channel, num_classes)\n",
    "if not pretrained:\n",
    "    shadow_trainloader = torch.utils.data.DataLoader(\n",
    "        shadow_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    shadow_testloader = torch.utils.data.DataLoader(\n",
    "        shadow_test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    optimizer = optim.SGD(shadow_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "    train_shadow_model(TARGET_PATH, device, shadow_model, shadow_trainloader, shadow_testloader, use_DP, noise, norm, loss, optimizer, delta)\n",
    "\n",
    "if test_models:\n",
    "    test_shadow_model, _ = target_model_fn(input_channel, num_classes)\n",
    "    test_shadow_model.load_state_dict(torch.load(TARGET_PATH + f\"{dataset_name}_{model_architecture}_shadow.pth\"))\n",
    "\n",
    "    shadow_trainloader = torch.utils.data.DataLoader(\n",
    "        shadow_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    shadow_testloader = torch.utils.data.DataLoader(\n",
    "        shadow_test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    optimizer = optim.SGD(test_shadow_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "    print(\"\\n-------------------------------------------------\")\n",
    "    test_shadow_model_train = shadow(shadow_trainloader, shadow_trainloader, test_shadow_model, device, use_DP, noise, norm, loss, optimizer, delta)\n",
    "    print(\"Shadow Model Train Acc:\", test_shadow_model_train.test())\n",
    "\n",
    "    test_shadow_model_test = shadow(shadow_testloader, shadow_testloader, test_shadow_model, device, use_DP, noise, norm, loss, optimizer, delta)\n",
    "    print(\"Shadow Model Test Acc:\", test_shadow_model_test.test())\n",
    "    print(\"\\n-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform statistical analysis to get the ranks of each layer, to understand what layer leaks the most information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the methods adds the noise to the initial input and \n",
    "# get the according output from the layer based on it\n",
    "def addNoiseInput(data, feature_vector):\n",
    "    global MEAN_DATA, STD_DATA\n",
    "\n",
    "    # data = torch.from_numpy(data)\n",
    "    # Noise is calculated by tensor + tensor's size * standard deviation + mean\n",
    "    noise_input = data + torch.randn(data.size()) * STD_DATA + MEAN_DATA\n",
    "    output = feature_vector(noise_input).flatten()\n",
    "\n",
    "    # output = feature_vector(noise_input).flatten()\n",
    "\n",
    "    return noise_input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method allows to get the output from specific layer based on the input\n",
    "\n",
    "# Condition = true when we only need correctly predicted label, \n",
    "# false = when we need prediction with true label of target_class\n",
    "def getTrainPredictions(model, shadow_in, shadow_out, target_layer, condition=True, noise=False):\n",
    "    model.eval()\n",
    "    output_train_in = []\n",
    "    output_train_out = []\n",
    "\n",
    "    noisy_output_train_in = []\n",
    "    noisy_output_train_out = []\n",
    "    \n",
    "    list_of_layers = list(model.children())\n",
    "    for j in (range(len(list_of_layers))):\n",
    "        if j != target_layer:\n",
    "            continue\n",
    "\n",
    "        if j == len(list_of_layers)-1:\n",
    "            feature_vector = model\n",
    "        else:\n",
    "            feature_vector = nn.Sequential(*list(model.children())[0:j+1]) # get first j layers\n",
    "\n",
    "        correct_by_feature_vector_in = 0\n",
    "        correct_by_model_in = 0\n",
    "        total_in = 0\n",
    "\n",
    "        correct_by_feature_vector_out = 0\n",
    "        correct_by_model_out = 0\n",
    "        total_out = 0\n",
    "\n",
    "        # Get predictions of train in and train out samples from target model\n",
    "        with torch.no_grad():\n",
    "            print(\"Getting predictions IN\")\n",
    "            in_loader = torch.utils.data.DataLoader(shadow_in, batch_size=1)\n",
    "\n",
    "            counter = 1\n",
    "            for og_data, og_target in in_loader:\n",
    "                # data, target = og_data.to(device), og_target.to(device) # send to device\n",
    "                data = og_data.clone()\n",
    "                target = og_target.clone()\n",
    "\n",
    "                total_in += 1\n",
    "                output_target = model(data)\n",
    "                _, pred_target = torch.topk(output_target, 1, dim=1, largest=True, sorted=True)\n",
    "                for i, t in enumerate(target):\n",
    "                    if t in pred_target[i]:\n",
    "                        correct_by_model_in += 1\n",
    "\n",
    "                output_feature = feature_vector(data)\n",
    "                _, pred_feature = torch.topk(output_feature, 1, dim=1, largest=True, sorted=True)\n",
    "                for i, t in enumerate(target):\n",
    "                    if t in pred_feature[i]:\n",
    "                        correct_by_feature_vector_in += 1\n",
    "\n",
    "                output_label = model(data)\n",
    "                _, pred = torch.topk(output_label, 1, dim=1, largest=True, sorted=True)                       \n",
    "\n",
    "                if condition:\n",
    "                    for i, t in enumerate(target):\n",
    "                        if t in pred[i]:\n",
    "                            original_output = feature_vector(data).flatten()\n",
    "                            output_train_in.append(original_output.cpu().numpy())\n",
    "\n",
    "                            if noise:\n",
    "                                noise_input, noise_output = addNoiseInput(og_data, feature_vector)\n",
    "                                # noisy_input_train_in.append(noise_input)\n",
    "                                noisy_output_train_in.append(noise_output.cpu().numpy())\n",
    "                else:\n",
    "                    for i, t in enumerate(target):\n",
    "                        original_output = feature_vector(data).flatten()\n",
    "                        output_train_in.append(original_output.cpu().numpy())\n",
    "\n",
    "                        if noise:\n",
    "                            noise_input, noise_output = addNoiseInput(og_data, feature_vector)\n",
    "                            # noisy_input_train_in.append(noise_input)\n",
    "                            noisy_output_train_in.append(noise_output.cpu().numpy())\n",
    "\n",
    "                data = None\n",
    "                target = None\n",
    "                clear_cache()\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    get_memory()\n",
    "                counter = counter + 1\n",
    "\n",
    "            print(\"Getting predictions OUT\")\n",
    "            out_loader = torch.utils.data.DataLoader(shadow_out, batch_size=1)\n",
    "\n",
    "            counter = 0\n",
    "            for og_data, og_target in out_loader:\n",
    "                # data, target = og_data.to(device), og_target.to(device) # send to device\n",
    "                data = og_data.clone()\n",
    "                target = og_target.clone()\n",
    "\n",
    "                total_out += 1\n",
    "                output_target = model(data)\n",
    "                _, pred_target = torch.topk(output_target, 1, dim=1, largest=True, sorted=True)\n",
    "                for i, t in enumerate(target):\n",
    "                    if t in pred_target[i]:\n",
    "                        correct_by_model_out += 1\n",
    "\n",
    "                output_feature = feature_vector(data)\n",
    "                _, pred_feature = torch.topk(output_feature, 1, dim=1, largest=True, sorted=True)\n",
    "                for i, t in enumerate(target):\n",
    "                    if t in pred_feature[i]:\n",
    "                        correct_by_feature_vector_out += 1\n",
    "\n",
    "                output_label = model(data)\n",
    "                _, pred = torch.topk(output_label, 1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "                if condition:\n",
    "                    for i, t in enumerate(target):\n",
    "                        if t in pred[i]:\n",
    "                            original_output = feature_vector(data).flatten()\n",
    "                            output_train_out.append(original_output.cpu().numpy())\n",
    "\n",
    "                            if noise:\n",
    "                                noise_input, noise_output = addNoiseInput(og_data, feature_vector)\n",
    "                                # noisy_input_train_out.append(noise_input)\n",
    "                                noisy_output_train_out.append(noise_output.cpu().numpy())\n",
    "                else:\n",
    "                    for i, t in enumerate(target):\n",
    "                        output = feature_vector(data).flatten()\n",
    "                        output_train_out.append(output.cpu().numpy())\n",
    "\n",
    "                        if noise:\n",
    "                                noise_input, noise_output = addNoiseInput(og_data, feature_vector)\n",
    "                                # noisy_input_train_out.append(noise_input)\n",
    "                                noisy_output_train_out.append(noise_output.cpu().numpy())\n",
    "\n",
    "                data = None\n",
    "                target = None\n",
    "                clear_cache()\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    get_memory()\n",
    "                counter = counter + 1\n",
    "\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Layer \", j, \" analysis\")\n",
    "        print(\"IN DATA. Correctly predicted by feature vector: \", correct_by_feature_vector_in)\n",
    "        print(\"IN DATA. Incorrectly predicted by feature vector: \", total_in - correct_by_feature_vector_in)\n",
    "        print(\"******************************\")\n",
    "\n",
    "        print(\"IN DATA. Correctly predicted by target model: \", correct_by_model_in)\n",
    "        print(\"IN DATA. Incorrectly predicted by target model: \", total_in - correct_by_model_in)\n",
    "        print(\"IN DATA. Total: \", total_in)\n",
    "\n",
    "        print(\"==============================\")\n",
    "        print(\"OUT DATA. Correctly predicted by feature vector: \", correct_by_feature_vector_out)\n",
    "        print(\"OUT DATA. Incorrectly predicted by feature vector: \", total_out - correct_by_feature_vector_out)\n",
    "        print(\"******************************\")\n",
    "\n",
    "        print(\"OUT DATA. Correctly predicted by target model: \", correct_by_model_out)\n",
    "        print(\"OUT DATA. Incorrectly predicted by target model: \", total_in - correct_by_model_out)\n",
    "        print(\"OUT DATA. Total: \", total_out)\n",
    "    \n",
    "    return output_train_in, output_train_out, noisy_output_train_in, noisy_output_train_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for KL divergence\n",
    "\n",
    "def compute_probs(data, n, min_n, max_n): \n",
    "    h, e = np.histogram(data, n, range=(min_n, max_n))\n",
    "    # print(\"shape \", len(data))\n",
    "    p = h/len(data)\n",
    "    return e, p\n",
    "\n",
    "def get_probs(list_of_tuples): \n",
    "    p = np.array([p[0] for p in list_of_tuples])\n",
    "    q = np.array([p[1] for p in list_of_tuples])\n",
    "    return p, q\n",
    "\n",
    "def support_intersection(p, q): \n",
    "    sup_int = (\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: (x[0]!=0) & (x[1]!=0), zip(p, q)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return sup_int\n",
    "\n",
    "def kl_divergence(p, q): \n",
    "    return np.sum(p*np.log(p/q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once certain methods p-values are calculated, we use Random Forest to get \n",
    "# the accuracy of the layer and rank it later\n",
    "def random_forest_layers_train(data_in, data_out, p_vals, layer_number):\n",
    "\n",
    "    layers_topk_data_in = []\n",
    "    layers_topk_data_out = []\n",
    "\n",
    "    number_of_neurons_in_layer = len(data_in[0])\n",
    "    layers_neurons_number = round((layers_neurons_percentage / 100) * number_of_neurons_in_layer)\n",
    "\n",
    "    top_vals = sorted(range(len(p_vals)), key=lambda i: p_vals[i], reverse=False)[:layers_neurons_number]\n",
    "\n",
    "    for i in top_vals:\n",
    "        layers_topk_data_in.append([row[i] for row in data_in])\n",
    "        layers_topk_data_out.append([row[i] for row in data_out])\n",
    "\n",
    "    # Train Random Forest on the output\n",
    "    all_X_train = np.vstack([np.array(data_in),np.array(data_out)])\n",
    "    all_y_train = np.vstack([np.ones((len(np.array(data_in)),1)),np.zeros((len(np.array(data_out)),1))])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_X_train, all_y_train, test_size=0.25, random_state=12)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train, y_train.ravel())\n",
    "    accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "    print(\"Layer \" + str (layer_number) + \" Accuracy \" + str(accuracy))\n",
    "\n",
    "    return accuracy, rf.feature_importances_, layers_topk_data_in, layers_topk_data_out, top_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for the logs\n",
    "print(\"-------> Dataset\", dataset_name)\n",
    "print(\"-------> METHOD\", method)\n",
    "print(\"-------> layers_neurons_percentage\", layers_neurons_percentage)\n",
    "print(\"-------> layers number \", number_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_neurons = []\n",
    "layer_numbers = []\n",
    "layers_neurons_number = 0\n",
    "intermediate_layer_input = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the shadow data and outputs the ranks of each layer with saved p-values\n",
    "def get_output_probs(shadow_model, shadow_in, shadow_out, method):\n",
    "    global target_layer\n",
    "    global layer_neurons\n",
    "    global number_of_layers\n",
    "    global layers_neurons_number\n",
    "    global intermediate_layer_input\n",
    "\n",
    "    list_of_layers = list(shadow_model.children())\n",
    "    j = len(list_of_layers) - 1\n",
    "\n",
    "    # Custom number of best layers\n",
    "    layers_scores = []\n",
    "    layers_vals = []\n",
    "\n",
    "    if recalculate:\n",
    "        for j in (range(0, len(list_of_layers))):\n",
    "            print(\"---> Layer \", j)\n",
    "            if not outputs_calculated:\n",
    "                # condition = True, when only considering correct predictions; False, when considering all \n",
    "                print(\"Calculating outputs\")\n",
    "                output_train_in, output_train_out, noisy_output_train_in, noisy_output_train_out = \\\n",
    "                    getTrainPredictions(shadow_model, shadow_in, shadow_out, j, condition=True, noise=True)\n",
    "\n",
    "                print(\"Calculated outputs\")\n",
    "                torch.save(noisy_output_train_in, f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_in_layer_{j}\")\n",
    "                torch.save(noisy_output_train_out, f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_out_layer_{j}\")\n",
    "\n",
    "                torch.save(output_train_in, f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_in_layer_{j}\")\n",
    "                torch.save(output_train_out, f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_out_layer_{j}\")\n",
    "\n",
    "                print(\"Saved Them\")\n",
    "\n",
    "                clear_cache()\n",
    "                get_memory()\n",
    "            \n",
    "            if not noise_difference_calculated:\n",
    "\n",
    "                noisy_output_train_in = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_in_layer_{j}\")\n",
    "                noisy_output_train_out = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_out_layer_{j}\")\n",
    "\n",
    "                output_train_in = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_in_layer_{j}\")\n",
    "                output_train_out = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_out_layer_{j}\")\n",
    "\n",
    "                print(\"Calculating Difference IN\")\n",
    "\n",
    "                diff_in = []\n",
    "                for l in range(len(output_train_in)):\n",
    "                    diff_in.append(abs(output_train_in[l] - noisy_output_train_in[l]))\n",
    "\n",
    "                clear_cache()\n",
    "                get_memory()\n",
    "\n",
    "                print(\"Calculating Difference OUT\")\n",
    "\n",
    "                diff_out = []\n",
    "                for l in range(len(output_train_out)):\n",
    "                    diff_out.append(abs(output_train_out[l] - noisy_output_train_out[l]))\n",
    "\n",
    "                print(\"output_train_in\", len(output_train_in), \" --- \", len(output_train_in[0]))\n",
    "                print(\"output_train_out\", len(output_train_out), \" --- \", len(output_train_out[0]))\n",
    "                print(\"noisy_output_train_in\", len(noisy_output_train_in), \" --- \", len(noisy_output_train_in[0]))\n",
    "                print(\"noisy_output_train_out\", len(noisy_output_train_out), \" --- \", len(noisy_output_train_out[0]))\n",
    "                print(\"difference_in\", len(diff_in), \" --- \", len(diff_in[0]))\n",
    "                print(\"difference_out\", len(diff_out), \" --- \", len(diff_out[0]))\n",
    "\n",
    "                print(\"Calculated Everything\")\n",
    "\n",
    "                torch.save(diff_in, f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/difference_in_layer_{j}\")\n",
    "                torch.save(diff_out, f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/difference_out_layer_{j}\")\n",
    "                \n",
    "                print(\"Saved\")\n",
    "\n",
    "                clear_cache()\n",
    "                get_memory()\n",
    "\n",
    "            output_train_in = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_in_layer_{j}\")\n",
    "            output_train_out = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_out_layer_{j}\")\n",
    "            \n",
    "            if method == \"RandomForest\":\n",
    "                all_X_train = np.vstack([np.array(output_train_in),np.array(output_train_out)])\n",
    "                all_y_train = np.vstack([np.ones((len(np.array(output_train_in)),1)),np.zeros((len(np.array(output_train_out)),1))])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(all_X_train, all_y_train, test_size=0.25, random_state=12)\n",
    "\n",
    "                rf = RandomForestClassifier(n_estimators=100)\n",
    "                rf.fit(X_train, y_train.ravel())\n",
    "                accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "                print(\"Layer \" + str (j) + \" Accuracy \" + str(accuracy))\n",
    "                layers_scores.append(accuracy)\n",
    "                layers_vals.append([rf.feature_importances_])\n",
    "            else:\n",
    "                p_vals = []\n",
    "                if method == \"Ttest\" or method == \"KS2Samp\" or method == \"Noise\" or \"JustNoise\":\n",
    "                    if method == \"Noise\":\n",
    "                        output_train_in = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/difference_in_layer_{j}\")\n",
    "                        output_train_out = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/difference_out_layer_{j}\")\n",
    "                    if method == \"JustNoise\":\n",
    "                        output_train_in = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_in_layer_{j}\")\n",
    "                        output_train_out = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_out_layer_{j}\")\n",
    "\n",
    "                    # Get P-values of neurons' outputs\n",
    "                    for i in range(len(output_train_in[0])):\n",
    "                        row_in = [row[i] for row in output_train_in]\n",
    "                        row_out = [row[i] for row in output_train_out]\n",
    "                        \n",
    "                        if method == \"Ttest\" or method == \"Noise\" or \"JustNoise\":\n",
    "                            stat, p = stats.ttest_ind(row_in, row_out, equal_var=False)\n",
    "                        elif method == \"KS2Samp\":\n",
    "                            stat, p = stats.ks_2samp(row_in, row_out)\n",
    "                        p_vals.append(p)\n",
    "\n",
    "                elif method == \"KLDivergence\":\n",
    "                    for i in range(len(output_train_in[0])):\n",
    "                        row_in = [row[i] for row in output_train_in]\n",
    "                        row_out = [row[i] for row in output_train_out]\n",
    "\n",
    "                        min_n = min(row_in+row_out)\n",
    "                        max_n = max(row_in+row_out)\n",
    "                        n_bins = 50\n",
    "                        e, p = compute_probs(row_in, n_bins, min_n, max_n)\n",
    "                        _, q = compute_probs(row_out, n_bins, min_n, max_n)                    \n",
    "\n",
    "                        list_of_tuples = support_intersection(p, q)\n",
    "                        p, q = get_probs(list_of_tuples)\n",
    "\n",
    "                        divergence = kl_divergence(p, q)\n",
    "                        p_vals.append(divergence)\n",
    "\n",
    "                elif method == \"Bootstrapping\":\n",
    "                    for i in range(len(output_train_in[0])):\n",
    "                        column_p = []\n",
    "\n",
    "                        col_in = [col[i] for col in output_train_in]\n",
    "                        col_out = [col[i] for col in output_train_out]\n",
    "\n",
    "                        # Random sample with replacement\n",
    "                        for c in range(5):\n",
    "                            sample_col_in = np.random.choice(col_in, replace=True, size=100)\n",
    "                            sample_col_out = np.random.choice(col_out, replace=True, size=100)\n",
    "\n",
    "                            stat, p = stats.ks_2samp(sample_col_in, sample_col_out)\n",
    "                            column_p.append(p)\n",
    "\n",
    "                        p_vals.append(mean(column_p))\n",
    "\n",
    "                print(\"Calculating RF accuracy\")\n",
    "                accuracy, feature_importances, layers_topk_data_in, layers_topk_data_out, top_vals = random_forest_layers_train(output_train_in, output_train_out, p_vals, j)\n",
    "                # layers_scores.append(accuracy)\n",
    "                # layers_vals.append([feature_importances])\n",
    "\n",
    "                print(\"Saving data of layer\")\n",
    "\n",
    "                if method != \"RandomForest\":\n",
    "                    torch.save(accuracy, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/accuracy_in_layer_{j}\")\n",
    "                    torch.save(feature_importances, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/feature_importances_in_layer_{j}\")\n",
    "                \n",
    "                torch.save(layers_topk_data_in, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/layers_topk_data_in_in_layer_{j}\")\n",
    "                torch.save(layers_topk_data_out, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/layers_topk_data_out_in_layer_{j}\")\n",
    "                torch.save(top_vals, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/top_vals_in_layer_{j}\")\n",
    "                print(\"Saved data\")     \n",
    "\n",
    "        print(\"Saving layers' values\")\n",
    "        if method != \"RandomForest\":\n",
    "            for j in (range(0, len(list_of_layers))):\n",
    "                layers_scores.append(torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/accuracy_in_layer_{j}\"))\n",
    "                layers_vals.append([torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/feature_importances_in_layer_{j}\")])\n",
    "\n",
    "        # Sort layers by ranks\n",
    "        layers_numbers = list(range(len(list_of_layers)))\n",
    "        sorted_layers_scores, sorted_layers_numbers = zip(*sorted(zip(layers_scores, layers_numbers)))\n",
    "        sorted_layers_numbers = sorted_layers_numbers[::-1]\n",
    "        sorted_layers_scores = sorted_layers_scores[::-1]\n",
    "\n",
    "        torch.save(sorted_layers_numbers, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/sorted_layers_numbers\")\n",
    "        torch.save(layers_vals, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/layers_vals\")\n",
    "        torch.save(sorted_layers_scores, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/sorted_layers_scores\")\n",
    "\n",
    "    sorted_layers_numbers = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/sorted_layers_numbers\")\n",
    "    layers_vals = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/layers_vals\")\n",
    "    sorted_layers_scores = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/sorted_layers_scores\")\n",
    "\n",
    "    print(\"\\nLayers Ranks\")\n",
    "    for i in range(len(sorted_layers_scores)):\n",
    "        print(\"Layer\", sorted_layers_numbers[i], \"  Score\", sorted_layers_scores[i])\n",
    "\n",
    "    top_values = []\n",
    "\n",
    "    for k in range(number_of_layers):\n",
    "        print(\"Getting Layer\", sorted_layers_numbers[k])\n",
    "\n",
    "        if sorted_layers_numbers[k] == len(list_of_layers) - 1:\n",
    "            print(\"Pass\")\n",
    "            k = k + 1\n",
    "\n",
    "        print(\"Getting Layer\", sorted_layers_numbers[k])\n",
    "\n",
    "        if method == \"RandomForest\":\n",
    "            output_train_in_all = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_in_layer_{sorted_layers_numbers[k]}\")\n",
    "\n",
    "            number_of_neurons_in_layer = len(output_train_in_all[0])\n",
    "            layers_neurons_number = round((layers_neurons_percentage / 100) * number_of_neurons_in_layer)\n",
    "\n",
    "            print(\"layers_neurons_number\", layers_neurons_number)\n",
    "            \n",
    "            layer_neurons.append(layers_neurons_number)\n",
    "            layer_numbers.append(sorted_layers_numbers[k])\n",
    "\n",
    "            top_neurons = sorted(range(len(layers_vals[sorted_layers_numbers[k]][0])), key=lambda i: layers_vals[sorted_layers_numbers[k]][0][i], reverse=True)[:layers_neurons_number]\n",
    "            top_values.append(top_neurons)\n",
    "        else:\n",
    "            if method == \"Noise\":\n",
    "                output_train_in_all = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/difference_in_layer_{sorted_layers_numbers[k]}\")\n",
    "            elif method == \"JustNoise\":\n",
    "                print(\"Hello\")\n",
    "                output_train_in_all = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/noisy_output_train_in_layer_{sorted_layers_numbers[k]}\")\n",
    "                print(\"output train in\", len(output_train_in_all[0]))\n",
    "            else:\n",
    "                output_train_in_all = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_in_layer_{sorted_layers_numbers[k]}\")\n",
    "                output_train_out_all = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/outputs/output_train_out_layer_{sorted_layers_numbers[k]}\")\n",
    "\n",
    "            number_of_neurons_in_layer = len(output_train_in_all[0])\n",
    "            layers_neurons_number = round((layers_neurons_percentage / 100) * number_of_neurons_in_layer)\n",
    "            print(\"layers_neurons_number\", layers_neurons_number)\n",
    "\n",
    "            layer_neurons.append(layers_neurons_number)\n",
    "            layer_numbers.append(sorted_layers_numbers[k])\n",
    "\n",
    "            top_neurons_preprocessed = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/top_vals_in_layer_{sorted_layers_numbers[k]}\")\n",
    "\n",
    "            if method == \"KLDivergence\":\n",
    "                top_neurons = sorted(range(len(top_neurons_preprocessed)), key=lambda i: top_neurons_preprocessed[i], reverse=True)[:layers_neurons_number]\n",
    "            else:\n",
    "                top_neurons = sorted(range(len(top_neurons_preprocessed)), key=lambda i: top_neurons_preprocessed[i], reverse=False)[:layers_neurons_number]\n",
    "                print(\"top neurons\")\n",
    "            \n",
    "            top_values.append(top_neurons)\n",
    "        \n",
    "        intermediate_layer_input = intermediate_layer_input + layers_neurons_number\n",
    "\n",
    "    # layer_neurons = new_layer_neurons\n",
    "    torch.save(sorted_layers_numbers[0:number_of_layers], f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/layers_numbers\")\n",
    "    torch.save(top_values, f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/top_values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No intermediate uses the base pipeline\n",
    "if method != \"NoIntermediate\":\n",
    "    test_shadow_model, _ = target_model_fn(input_channel, num_classes)\n",
    "    test_shadow_model.load_state_dict(torch.load(TARGET_PATH + f\"{dataset_name}_{model_architecture}_shadow.pth\"))\n",
    "    get_output_probs(test_shadow_model, shadow_train, shadow_test, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method assigns 0s and 1s depending on the relation to the training\n",
    "def get_attack_dataset_with_shadow(target_train, target_test, shadow_train, shadow_test, batch_size):\n",
    "    mem_train, nonmem_train, mem_test, nonmem_test = list(shadow_train), list(shadow_test), list(target_train), list(target_test)\n",
    "\n",
    "    for i in range(len(mem_train)):\n",
    "        mem_train[i] = mem_train[i] + (1,)\n",
    "    for i in range(len(nonmem_train)):\n",
    "        nonmem_train[i] = nonmem_train[i] + (0,)\n",
    "    for i in range(len(nonmem_test)):\n",
    "        nonmem_test[i] = nonmem_test[i] + (0,)\n",
    "    for i in range(len(mem_test)):\n",
    "        mem_test[i] = mem_test[i] + (1,)\n",
    "\n",
    "    train_length = min(len(mem_train), len(nonmem_train))\n",
    "    test_length = min(len(mem_test), len(nonmem_test))\n",
    "\n",
    "    mem_train, _ = torch.utils.data.random_split(mem_train, [train_length, len(mem_train) - train_length])\n",
    "    non_mem_train, _ = torch.utils.data.random_split(nonmem_train, [train_length, len(nonmem_train) - train_length])\n",
    "    mem_test, _ = torch.utils.data.random_split(mem_test, [test_length, len(mem_test) - test_length])\n",
    "    non_mem_test, _ = torch.utils.data.random_split(nonmem_test, [test_length, len(nonmem_test) - test_length])\n",
    "    \n",
    "    attack_train = mem_train + non_mem_train\n",
    "    attack_test = mem_test + non_mem_test\n",
    "\n",
    "    attack_trainloader = torch.utils.data.DataLoader(\n",
    "        attack_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    attack_testloader = torch.utils.data.DataLoader(\n",
    "        attack_test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return attack_trainloader, attack_testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_size(model):\n",
    "    gradient_size = []\n",
    "    gradient_list = reversed(list(model.named_parameters()))\n",
    "    for name, parameter in gradient_list:\n",
    "        if 'weight' in name:\n",
    "            gradient_size.append(parameter.shape)\n",
    "\n",
    "    return gradient_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the attack model and definition of its architecture\n",
    "class WhiteBoxAttackModel(nn.Module):\n",
    "    def __init__(self, class_num, total, intermediate_layer_input):\n",
    "        super(WhiteBoxAttackModel, self).__init__()\n",
    "\n",
    "        self.intermediate_layer_input = intermediate_layer_input\n",
    "\n",
    "        self.Intermediate_Output_Component_result = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            # nn.Linear(class_num, 128),\n",
    "            nn.Linear(self.intermediate_layer_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "        )\n",
    "\n",
    "        self.Output_Component = nn.Sequential(\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(class_num, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t)\n",
    "\n",
    "        self.Loss_Component = nn.Sequential(\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t)\n",
    "\n",
    "        self.Gradient_Component = nn.Sequential(\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Conv2d(1, 1, kernel_size=5, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(total, 256),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(256, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t)\n",
    "\n",
    "        self.Label_Component = nn.Sequential(\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(class_num, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t)\n",
    "        \n",
    "        self.Encoder_Component = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(320, 256),\n",
    "\t\t\tnn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(256, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, 2),\n",
    "\t\t)\n",
    "\n",
    "\n",
    "    def forward(self, intermediate_output, output, loss, gradient, label):\n",
    "\n",
    "        if method != \"NoIntermediate\":\n",
    "            try:\n",
    "                intermediate_output = intermediate_output.to(device)\n",
    "                self.intermediate_layer_input = intermediate_output.shape[0]\n",
    "\n",
    "                self.Intermediate_Output_Component_result = nn.Sequential(\n",
    "                    nn.Dropout(p=0.2),\n",
    "                    nn.Linear(self.intermediate_layer_input, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                )\n",
    "                self.Intermediate_Output_Component_result = self.Intermediate_Output_Component_result.to(device)\n",
    "                \n",
    "                Intermediate_Output_Component_result = self.Intermediate_Output_Component_result(intermediate_output)\n",
    "            except:\n",
    "                intermediate_output = np.transpose(intermediate_output.cpu().detach().numpy())\n",
    "                intermediate_output = torch.squeeze(torch.Tensor(intermediate_output), 1)\n",
    "                intermediate_output = intermediate_output.to(device)\n",
    "\n",
    "                self.intermediate_layer_input = intermediate_output.shape[1]\n",
    "\n",
    "                self.Intermediate_Output_Component_result = nn.Sequential(\n",
    "                    nn.Dropout(p=0.2),\n",
    "                    nn.Linear(self.intermediate_layer_input, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                )\n",
    "                self.Intermediate_Output_Component_result = self.Intermediate_Output_Component_result.to(device)\n",
    "                \n",
    "                Intermediate_Output_Component_result = self.Intermediate_Output_Component_result(intermediate_output)\n",
    "        else:\n",
    "            Output_Component_result = self.Output_Component(output)\n",
    "\n",
    "        Loss_Component_result = self.Loss_Component(loss)\n",
    "        Gradient_Component_result = self.Gradient_Component(gradient)\n",
    "        Label_Component_result = self.Label_Component(label)\n",
    "\n",
    "        Output_Component_result = self.Output_Component(output)\n",
    "        final_inputs = torch.cat((Intermediate_Output_Component_result, Output_Component_result, Loss_Component_result, Gradient_Component_result, Label_Component_result), 1)\n",
    "\n",
    "        final_result = self.Encoder_Component(final_inputs)\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.normal_(m.weight.data)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif isinstance(m,nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class for the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attack_for_whitebox():\n",
    "    def __init__(self, TARGET_PATH, SHADOW_PATH, ATTACK_SETS, attack_train_loader, attack_test_loader, target_model, shadow_model, attack_model, device, class_num):\n",
    "        self.device = device\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.ATTACK_SETS = ATTACK_SETS\n",
    "\n",
    "        self.TARGET_PATH = TARGET_PATH\n",
    "        self.target_model = target_model.to(self.device)\n",
    "        self.target_model.load_state_dict(torch.load(self.TARGET_PATH))\n",
    "        self.target_model.eval()\n",
    "\n",
    "\n",
    "        self.SHADOW_PATH = SHADOW_PATH\n",
    "        self.shadow_model = shadow_model.to(self.device)\n",
    "        self.shadow_model.load_state_dict(torch.load(self.SHADOW_PATH))\n",
    "        self.shadow_model.eval()\n",
    "\n",
    "        self.attack_train_loader = attack_train_loader\n",
    "        self.attack_test_loader = attack_test_loader\n",
    "\n",
    "        self.attack_model = attack_model.to(self.device)\n",
    "        torch.manual_seed(0)\n",
    "        self.attack_model.apply(weights_init)\n",
    "\n",
    "        self.target_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.attack_criterion = nn.CrossEntropyLoss()\n",
    "        #self.optimizer = optim.SGD(self.attack_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "        self.optimizer = optim.Adam(self.attack_model.parameters(), lr=1e-5)\n",
    "\n",
    "        self.attack_train_data = None\n",
    "        self.attack_test_data = None\n",
    "        \n",
    "    # This method allows to get only certain neurons for each layer's output based on the method\n",
    "    def _get_data(self, model, inputs, targets):\n",
    "        global method\n",
    "        global layers_neurons_number\n",
    "        global layer_numbers\n",
    "        global layer_neurons\n",
    "\n",
    "        intermediate_output = []\n",
    "        results = []\n",
    "\n",
    "        if method != \"NoIntermediate\":\n",
    "            list_of_layers = list(shadow_model.children())\n",
    "            \n",
    "            index_layer = 0\n",
    "            layers_vals = torch.load(f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/outputs/top_values\")\n",
    "\n",
    "            for j in layer_numbers:\n",
    "                temporary_output = []\n",
    "\n",
    "                if j == len(list_of_layers)-1:\n",
    "                        feature_vector = model\n",
    "                else:\n",
    "                    feature_vector = nn.Sequential(*list(model.children())[0:j+1]) # get first j layers\n",
    "\n",
    "                if method == \"RandomForest\":\n",
    "                    for i in range(len(inputs)):\n",
    "                        input = inputs[i]\n",
    "                        squeezed_input = torch.unsqueeze(input, 0)\n",
    "                        output_vector = feature_vector(squeezed_input)\n",
    "                        output = output_vector.flatten()\n",
    "                        temporary_output.append(output.cpu().detach().numpy())\n",
    "\n",
    "                    top_neurons = sorted(range(len(layers_vals[index_layer])), key=lambda i: layers_vals[index_layer][i], reverse=True)[:layer_neurons[index_layer]]\n",
    "                    index_layer = index_layer + 1\n",
    "                    for i in top_neurons:\n",
    "                        intermediate_output.append([row[i] for row in temporary_output])\n",
    "                else:\n",
    "                    noisy_output = []\n",
    "                    difference = []\n",
    "\n",
    "                    for i in range(len(inputs)):\n",
    "                        if method == \"JustNoise\":\n",
    "                            noise_input = inputs[i] + torch.randn(inputs[i].cpu().size()).to(device) * STD_DATA + MEAN_DATA\n",
    "                            squeezed_input = torch.unsqueeze(noise_input, 0)\n",
    "                            output_vector = feature_vector(squeezed_input)\n",
    "                            noise_output = output_vector.cuda().flatten()\n",
    "                            temporary_output.append(noise_output.cpu().detach().numpy())\n",
    "                        else:\n",
    "                            input = inputs[i]\n",
    "                            squeezed_input = torch.unsqueeze(input, 0)\n",
    "                            output_vector = feature_vector(squeezed_input)\n",
    "                            og_output = output_vector.flatten()\n",
    "                            temporary_output.append(og_output.cpu().detach().numpy())\n",
    "\n",
    "                            if method == \"Noise\":\n",
    "                                noise_input = inputs[i] + torch.randn(inputs[i].cpu().size()).to(device) * STD_DATA + MEAN_DATA\n",
    "                                squeezed_input = torch.unsqueeze(noise_input, 0)\n",
    "                                output_vector = feature_vector(squeezed_input)\n",
    "                                noise_output = output_vector.flatten()\n",
    "\n",
    "                                noisy_output.append(noise_output.cpu().detach().numpy())\n",
    "\n",
    "                                diff = []\n",
    "                                for l in range(len(og_output)):\n",
    "                                    diff.append(abs(og_output[l] - noise_output[l]))\n",
    "                                difference.append(diff)\n",
    "                            \n",
    "\n",
    "                    if method == \"Noise\":\n",
    "                        temporary_output = difference\n",
    "\n",
    "                    top_neurons = sorted(range(len(layers_vals[index_layer])), key=lambda i: layers_vals[index_layer][i], reverse=True)[:layer_neurons[index_layer]]\n",
    "\n",
    "                    if method == \"KLDivergence\":\n",
    "                        top_neurons = sorted(range(len(top_neurons)), key=lambda i: top_neurons[i], reverse=True)[:layer_neurons[index_layer]]\n",
    "                    else:\n",
    "                        top_neurons = sorted(range(len(top_neurons)), key=lambda i: top_neurons[i], reverse=False)[:layer_neurons[index_layer]]\n",
    "\n",
    "                    index_layer = index_layer + 1\n",
    "                    for i in top_neurons:\n",
    "                        intermediate_output.append([row[i] for row in temporary_output])\n",
    "\n",
    "            intermediate_output = torch.tensor(intermediate_output, requires_grad=True).to(device)\n",
    "            intermediate_output, _ = torch.sort(intermediate_output, descending=True)\n",
    "            intermediate_output = intermediate_output.to(device)\n",
    "        \n",
    "        results = model(inputs)\n",
    "        # outputs = F.softmax(outputs, dim=1)\n",
    "        losses = self.target_criterion(results, targets)\n",
    "\n",
    "        gradients = []\n",
    "        \n",
    "        for loss in losses:\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            gradient_list = reversed(list(model.named_parameters()))\n",
    "\n",
    "            for name, parameter in gradient_list:\n",
    "                if 'weight' in name:\n",
    "                    gradient = parameter.grad.clone() # [column[:, None], row].resize_(100,100)\n",
    "                    gradient = gradient.unsqueeze_(0)\n",
    "                    gradients.append(gradient.unsqueeze_(0))\n",
    "                    break\n",
    "\n",
    "        labels = []\n",
    "        for num in targets:\n",
    "            label = [0 for i in range(self.class_num)]\n",
    "            label[num.item()] = 1\n",
    "            labels.append(label)\n",
    "\n",
    "        gradients = torch.cat(gradients, dim=0)\n",
    "        losses = losses.unsqueeze_(1).detach()\n",
    "        outputs, _ = torch.sort(results, descending=True)\n",
    "        labels = torch.Tensor(labels)\n",
    "\n",
    "        return intermediate_output, outputs, losses, gradients, labels\n",
    "\n",
    "    # This method allows to get \n",
    "    def prepare_dataset(self):\n",
    "        print(\"Preparing Train Dataset\")\n",
    "\n",
    "        # k = 0\n",
    "        with open(self.ATTACK_SETS + \"train.p\", \"wb\") as f:\n",
    "            for inputs, targets, members in self.attack_train_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                intermediate_output, output, loss, gradient, label = self._get_data(self.shadow_model, inputs, targets)\n",
    "                pickle.dump((intermediate_output, output, loss, gradient, label, members), f)\n",
    "\n",
    "        print(\"Finished Saving Train Dataset\")\n",
    "\n",
    "        print(\"Preparing Test Dataset\")\n",
    "\n",
    "        k = 0\n",
    "        with open(self.ATTACK_SETS + \"test.p\", \"wb\") as f:\n",
    "            for inputs, targets, members in self.attack_test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                intermediate_output, output, loss, gradient, label = self._get_data(self.target_model, inputs, targets)\n",
    "                pickle.dump((intermediate_output, output, loss, gradient, label, members), f)\n",
    "        print(\"Finished Saving Test Dataset\")\n",
    "\n",
    "    def train(self, epoch, result_path):\n",
    "        self.attack_model.train()\n",
    "        batch_idx = 1\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        final_train_gndtrth = []\n",
    "        final_train_predict = []\n",
    "        final_train_probabe = []\n",
    "\n",
    "        final_result = []\n",
    "\n",
    "        with open(self.ATTACK_SETS + \"train.p\", \"rb\") as f:\n",
    "            while(True):\n",
    "                try: \n",
    "                    intermediate_output, output, loss, gradient, label, members = pickle.load(f)\n",
    "                    output, loss, gradient, label, members = output.to(self.device), loss.to(self.device), gradient.to(self.device), label.to(self.device), members.to(self.device)\n",
    "                    results = self.attack_model(intermediate_output, output, loss, gradient, label)\n",
    "                    losses = self.attack_criterion(results, members)\n",
    "                    \n",
    "                    losses.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    train_loss += losses.item()\n",
    "                    _, predicted = results.max(1)\n",
    "                    total += members.size(0)\n",
    "                    correct += predicted.eq(members).sum().item()\n",
    "\n",
    "                    if epoch:\n",
    "                        final_train_gndtrth.append(members)\n",
    "                        final_train_predict.append(predicted)\n",
    "                        final_train_probabe.append(results[:, 1])\n",
    "\n",
    "                    batch_idx += 1\n",
    "                except EOFError:\n",
    "                    break\t\n",
    "\n",
    "        if epoch:\n",
    "            final_train_gndtrth = torch.cat(final_train_gndtrth, dim=0).cpu().detach().numpy()\n",
    "            final_train_predict = torch.cat(final_train_predict, dim=0).cpu().detach().numpy()\n",
    "            final_train_probabe = torch.cat(final_train_probabe, dim=0).cpu().detach().numpy()\n",
    "\n",
    "            conf_matrix =  confusion_matrix(final_train_gndtrth, final_train_predict)\n",
    "\n",
    "            train_f1_score = f1_score(final_train_gndtrth, final_train_predict)\n",
    "            train_roc_auc_score = roc_auc_score(final_train_gndtrth, final_train_probabe)\n",
    "\n",
    "            final_result.append(train_f1_score)\n",
    "            final_result.append(train_roc_auc_score)\n",
    "\n",
    "            with open(result_path, \"wb\") as f:\n",
    "                pickle.dump((final_train_gndtrth, final_train_predict, final_train_probabe), f)\n",
    "            \n",
    "            print(\"Saved Attack Train Ground Truth and Predict Sets\")\n",
    "            print(\"Train F1: %f\\nAUC: %f\" % (train_f1_score, train_roc_auc_score))\n",
    "\n",
    "            print(\"\\n\\n\\n\\n\")\n",
    "            tn, fp, fn, tp = conf_matrix.ravel()\n",
    "            print( 'Train Acc: %.3f%% (%d/%d)' % (100.*correct/(1.0*total), correct, total))\n",
    "            print(\"Recall:\", str(round((tp/(tp+fn)) * 100, 2)))\n",
    "            print(\"Negative Recall:\", str(round((tn/(tn+fp)) * 100, 2)))\n",
    "            print(\"Train F1: %f\\nAUC: %f\" % (train_f1_score, train_roc_auc_score))\n",
    "            print(\"TP: \", tp, \"FP\", fp, \"TN\", tn, \"FN\", fn)\n",
    "\n",
    "        try:\n",
    "            final_result.append(1.*correct/total)\n",
    "        except:\n",
    "            final_result.append(1.*correct/1)\n",
    "            \n",
    "        print( 'Train Acc: %.3f%% (%d/%d) | Loss: %.3f' % (100.*correct/total, correct, total, 1.*train_loss/batch_idx))\n",
    "\n",
    "        return final_result\n",
    "\n",
    "\n",
    "    def test(self, epoch, result_path):\n",
    "        self.attack_model.eval()\n",
    "        batch_idx = 1\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        final_test_gndtrth = []\n",
    "        final_test_predict = []\n",
    "        final_test_probabe = []\n",
    "\n",
    "        final_result = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with open(self.ATTACK_SETS + \"test.p\", \"rb\") as f:\n",
    "                while(True):\n",
    "                    try:\n",
    "                        intermediate_output, output, loss, gradient, label, members = pickle.load(f)\n",
    "                        output, loss, gradient, label, members = output.to(self.device), loss.to(self.device), gradient.to(self.device), label.to(self.device), members.to(self.device)\n",
    "\n",
    "                        results = self.attack_model(intermediate_output, output, loss, gradient, label)\n",
    "\n",
    "                        _, predicted = results.max(1)\n",
    "                        total += members.size(0)\n",
    "                        correct += predicted.eq(members).sum().item()\n",
    "\n",
    "                        results = F.softmax(results, dim=1)\n",
    "\n",
    "                        if epoch:\n",
    "                            final_test_gndtrth.append(members)\n",
    "                            final_test_predict.append(predicted)\n",
    "                            final_test_probabe.append(results[:, 1])\n",
    "\n",
    "                        batch_idx += 1\n",
    "                    except EOFError:\n",
    "                        break\n",
    "\n",
    "        if epoch:\n",
    "            final_test_gndtrth = torch.cat(final_test_gndtrth, dim=0).cpu().numpy()\n",
    "            final_test_predict = torch.cat(final_test_predict, dim=0).cpu().numpy()\n",
    "            final_test_probabe = torch.cat(final_test_probabe, dim=0).cpu().numpy()\n",
    "            \n",
    "            conf_matrix = confusion_matrix(final_test_gndtrth, final_test_predict)  \n",
    "\n",
    "            test_f1_score = f1_score(final_test_gndtrth, final_test_predict)\n",
    "            test_roc_auc_score = roc_auc_score(final_test_gndtrth, final_test_probabe)\n",
    "\n",
    "            final_result.append(test_f1_score)\n",
    "            final_result.append(test_roc_auc_score)\n",
    "\n",
    "\n",
    "            with open(result_path, \"wb\") as f:\n",
    "                pickle.dump((final_test_gndtrth, final_test_predict, final_test_probabe), f)\n",
    "\n",
    "            print(\"Saved Attack Test Ground Truth and Predict Sets\")\n",
    "            print(\"Test F1: %f\\nAUC: %f\" % (test_f1_score, test_roc_auc_score))\n",
    "\n",
    "            print(\"\\n\\n\\n\\n\")\n",
    "            tn, fp, fn, tp = conf_matrix.ravel()\n",
    "            print( 'Test Acc: %.3f%% (%d/%d)' % (100.*correct/(1.0*total), correct, total))\n",
    "            print(\"Recall:\", str(round((tp/(tp+fn)) * 100, 2)))\n",
    "            print(\"Negative Recall:\", str(round((tn/(tn+fp)) * 100, 2)))\n",
    "            print(\"Test F1: %f\\nAUC: %f\" % (test_f1_score, test_roc_auc_score))\n",
    "            print(\"TP: \", tp, \"FP\", fp, \"TN\", tn, \"FN\", fn)\n",
    "\n",
    "        final_result.append(1.*correct/total)\n",
    "        print( 'Test Acc: %.3f%% (%d/%d)' % (100.*correct/(1.0*total), correct, total))\n",
    "\n",
    "        return final_result\n",
    "\n",
    "    def delete_pickle(self):\n",
    "        train_file = glob.glob(self.ATTACK_SETS +\"train.p\")\n",
    "        for trf in train_file:\n",
    "            os.remove(trf)\n",
    "\n",
    "        test_file = glob.glob(self.ATTACK_SETS +\"test.p\")\n",
    "        for tef in test_file:\n",
    "            os.remove(tef)\n",
    "\n",
    "    def saveModel(self, path):\n",
    "        torch.save(self.attack_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the attaack using shadow model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_mode3(TARGET_PATH, SHADOW_PATH, ATTACK_PATH, device, attack_trainloader, attack_testloader, target_model, shadow_model, attack_model, get_attack_set, num_classes):\n",
    "    MODELS_PATH = ATTACK_PATH + \"_meminf_attack3.pth\"\n",
    "    RESULT_PATH = ATTACK_PATH + \"_meminf_attack3.p\"\n",
    "    ATTACK_SETS = ATTACK_PATH + \"_meminf_attack_mode3_\"\n",
    "\n",
    "    attack = attack_for_whitebox(TARGET_PATH, SHADOW_PATH, ATTACK_SETS, attack_trainloader, attack_testloader, target_model, shadow_model, attack_model, device, num_classes)\n",
    "    \n",
    "    if not prepare_dataset_pretrained:\n",
    "        print(\"Preparing attack dataset\")\n",
    "        attack.delete_pickle()\n",
    "        attack.prepare_dataset()\n",
    "\n",
    "    for i in range(50):\n",
    "        flag = 1 if i == 49 else 0\n",
    "        print(\"Epoch %d :\" % (i+1))\n",
    "        res_train = attack.train(flag, RESULT_PATH)\n",
    "        res_test = attack.test(flag, RESULT_PATH)\n",
    "\n",
    "    attack.saveModel(MODELS_PATH)\n",
    "    print(\"Saved Attack Model\")\n",
    "\n",
    "    return res_train, res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_meminf(PATH, device, num_classes, target_train, target_test, shadow_train, shadow_test, target_model, shadow_model, use_DP, noise, norm, delta):\n",
    "    global intermediate_layer_input\n",
    "\n",
    "    batch_size = 64\n",
    "    \n",
    "    attack_trainloader, attack_testloader = get_attack_dataset_with_shadow(\n",
    "        target_train, target_test, shadow_train, shadow_test, batch_size)\n",
    "\n",
    "    #for white box\n",
    "    gradient_size = get_gradient_size(target_model)\n",
    "    total = gradient_size[0][0] // 2 * gradient_size[0][1] // 2\n",
    "\n",
    "    attack_model = WhiteBoxAttackModel(num_classes, total, intermediate_layer_input)\n",
    "\n",
    "    ATTACK_PATH = f\"{root_folder}attack/{dataset_name}/{model_architecture}/{method}/\"\n",
    "    \n",
    "    attack_mode3(PATH + f\"{dataset_name}_{model_architecture}_target.pth\", \\\n",
    "        PATH + f\"{dataset_name}_{model_architecture}_shadow.pth\", ATTACK_PATH, device, \\\n",
    "        attack_trainloader, attack_testloader, target_model, shadow_model, attack_model, 1, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model, _ = target_model_fn(input_channel, num_classes)\n",
    "shadow_model, _ = target_model_fn(input_channel, num_classes)\n",
    "test_meminf(TARGET_PATH, device, num_classes, target_train, target_test, shadow_train, shadow_test, target_model, shadow_model, use_DP, noise, norm, delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88917169739e9e2f58186ae9eba621769234d21ef1ad643a1632d5939a675b5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
